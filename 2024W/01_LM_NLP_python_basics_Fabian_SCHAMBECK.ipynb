{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1OqX9GeN216c"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fubotz/ML_NLP_ws2024-fs/blob/main/01_LM_NLP_python_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8FCi_UAtCN"
      },
      "source": [
        "# Python Basics\n",
        "\n",
        "In this exercise, we'll explore python NLP capabilities with the help of the package `nltk`.\n",
        "\n",
        "By the end of the exercise, you will:\n",
        "* be introduced to the nltk package and its functionality\n",
        "* understand the basics of text analysis, and know how to approach this unstructured data.\n",
        "* Understand the terms 'n-gram' & 'collocation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF7evQxY31Kr"
      },
      "source": [
        "We are going to use the package `NLTK` - 'Natural Language Toolkit' (https://www.nltk.org/).\n",
        "\n",
        "NLTK is a great package for research and for learning. However, it isn't recommended for production use and for real-world applications, as it isn't fast enough and therefore doesn't scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OqX9GeN216c"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS2o9QMaUmzC"
      },
      "source": [
        "import random\n",
        "\n",
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gBd4ZgXzAmP",
        "outputId": "7effdcec-0fd2-4fd8-df66-f27c6c9f38b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('book')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRzGNH63xcwQ",
        "outputId": "cf5b2e9f-1cd6-4210-b259-bc29ce3f6e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "J_dcD1AYhBez"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlR3eNPu3EWS"
      },
      "source": [
        "## A Closer Look at Python: Texts as Lists of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul9GAlK63sMj"
      },
      "source": [
        "We will use the great book 'Moby Dick' by Herman Melville, as our learning experiment playground.\n",
        "\n",
        "The book is already tokenized and stored as a list of these tokens, under a variable with the excellent, well expressed name - `text1` (please do yourself - and me - a favor and name your variables in a more meaningful manner than that...).\n",
        "\n",
        "We start - as you should always do - with exploring and looking at our dataset.\n",
        "\n",
        "Let's peek at the first 100 words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjIvrcow3Pjc",
        "outputId": "76e06f2b-ac11-4977-d074-08bbecea5690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text1[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Moby',\n",
              " 'Dick',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " '1851',\n",
              " ']',\n",
              " 'ETYMOLOGY',\n",
              " '.',\n",
              " '(',\n",
              " 'Supplied',\n",
              " 'by',\n",
              " 'a',\n",
              " 'Late',\n",
              " 'Consumptive',\n",
              " 'Usher',\n",
              " 'to',\n",
              " 'a',\n",
              " 'Grammar',\n",
              " 'School',\n",
              " ')',\n",
              " 'The',\n",
              " 'pale',\n",
              " 'Usher',\n",
              " '--',\n",
              " 'threadbare',\n",
              " 'in',\n",
              " 'coat',\n",
              " ',',\n",
              " 'heart',\n",
              " ',',\n",
              " 'body',\n",
              " ',',\n",
              " 'and',\n",
              " 'brain',\n",
              " ';',\n",
              " 'I',\n",
              " 'see',\n",
              " 'him',\n",
              " 'now',\n",
              " '.',\n",
              " 'He',\n",
              " 'was',\n",
              " 'ever',\n",
              " 'dusting',\n",
              " 'his',\n",
              " 'old',\n",
              " 'lexicons',\n",
              " 'and',\n",
              " 'grammars',\n",
              " ',',\n",
              " 'with',\n",
              " 'a',\n",
              " 'queer',\n",
              " 'handkerchief',\n",
              " ',',\n",
              " 'mockingly',\n",
              " 'embellished',\n",
              " 'with',\n",
              " 'all',\n",
              " 'the',\n",
              " 'gay',\n",
              " 'flags',\n",
              " 'of',\n",
              " 'all',\n",
              " 'the',\n",
              " 'known',\n",
              " 'nations',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " '.',\n",
              " 'He',\n",
              " 'loved',\n",
              " 'to',\n",
              " 'dust',\n",
              " 'his',\n",
              " 'old',\n",
              " 'grammars',\n",
              " ';',\n",
              " 'it',\n",
              " 'somehow',\n",
              " 'mildly',\n",
              " 'reminded',\n",
              " 'him',\n",
              " 'of',\n",
              " 'his',\n",
              " 'mortality',\n",
              " '.',\n",
              " '\"',\n",
              " 'While',\n",
              " 'you',\n",
              " 'take',\n",
              " 'in',\n",
              " 'hand',\n",
              " 'to',\n",
              " 'school',\n",
              " 'others',\n",
              " ',']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD5ADOnC48oM"
      },
      "source": [
        "Pay attention that punctuations here are also conisdered as a `token`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uthWKmm153r-"
      },
      "source": [
        "## Exercise #1: Show the last 23 tokens in the book:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvWRtYl-3Ubi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2302e8-37c0-4ab8-e85a-80665a1a2446"
      },
      "source": [
        "### YOUR TURN:\n",
        "### Write a code that shows the last sentence (23 tokens) of the book\n",
        "\n",
        "print(text1[-23:])\n",
        "\n",
        "### End"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'was', 'the', 'devious', '-', 'cruising', 'Rachel', ',', 'that', 'in', 'her', 'retracing', 'search', 'after', 'her', 'missing', 'children', ',', 'only', 'found', 'another', 'orphan', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC0w4C5T68wF"
      },
      "source": [
        "## Lists vs Sets\n",
        "\n",
        "In python, an ordered set, with repetition, is defined as a `List`, and is defied by sqaured braces [].\n",
        "\n",
        "An unordered set, where repetitions are *discarded*, is defined with curly braces: {}.\n",
        "\n",
        "When converting a list into a set, we can get the **vocabulary** of the corpus, the *unique* words that the dataset is constructed of:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsUBYcwl6Cdc",
        "outputId": "27bf98c1-8dcc-43a2-c612-2631045fab8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = set(text1)\n",
        "\n",
        "# We can't get the 'last 25 words', since there is no order...\n",
        "# But we can convert it into a list first, and even sort it\n",
        "\n",
        "list(sorted(vocab))[-50:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yawned',\n",
              " 'yawning',\n",
              " 'ye',\n",
              " 'yea',\n",
              " 'year',\n",
              " 'yearly',\n",
              " 'years',\n",
              " 'yeast',\n",
              " 'yell',\n",
              " 'yelled',\n",
              " 'yelling',\n",
              " 'yellow',\n",
              " 'yellowish',\n",
              " 'yells',\n",
              " 'yes',\n",
              " 'yesterday',\n",
              " 'yet',\n",
              " 'yield',\n",
              " 'yielded',\n",
              " 'yielding',\n",
              " 'yields',\n",
              " 'yoke',\n",
              " 'yoked',\n",
              " 'yokes',\n",
              " 'yoking',\n",
              " 'yon',\n",
              " 'yonder',\n",
              " 'yore',\n",
              " 'you',\n",
              " 'young',\n",
              " 'younger',\n",
              " 'youngest',\n",
              " 'youngish',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourselbs',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'youth',\n",
              " 'youthful',\n",
              " 'zag',\n",
              " 'zay',\n",
              " 'zeal',\n",
              " 'zephyr',\n",
              " 'zig',\n",
              " 'zodiac',\n",
              " 'zone',\n",
              " 'zoned',\n",
              " 'zones',\n",
              " 'zoology']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkvijxKv8nqm"
      },
      "source": [
        "## Exercise #2: Vocabulary Length\n",
        "\n",
        "How many words does our vocabulary contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJnFJcsG8k2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4b902c-7d50-4f97-c665-c91ff199208d"
      },
      "source": [
        "### YOUR TURN:\n",
        "### Write python code that prints the size of Moby Dick book's vocabulary\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "### End"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxlwVLXG9BRg"
      },
      "source": [
        "# Text Analysis: Frequency Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlpuCzpK9H9J"
      },
      "source": [
        "[nltk](http://www.nltk.org) is a library with many research tools for probabilistic information and dataset exploration.\n",
        "\n",
        "For example, it includes a function, `FreqDist`, that return the probability of the occurance of a word in a text:\n",
        "\n",
        "http://www.nltk.org/api/nltk.html?highlight=freqdist#module-nltk.probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z76z5LY19FQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42a6166-4469-40d3-e61f-eeeb2afda08d"
      },
      "source": [
        "### YOUR TURN:\n",
        "## 1) Write python function named `get_most_frequent(n: int)` that calculates the frequency of words in text1 and returns the top n common ones (n is given as a parameter).\n",
        "## 2) Write a python function - `get_frequency(words: list[str])` that given a list of words, prints the frequency of each of those words in text1.\n",
        "## 3) Use the functions to print how many times the words 'with', 'Moby', 'fish' and 'whale' appear in the book.\n",
        "## hint: FreqDist is a smart python dictionary that already has methods for these tasks, such as .most_common()\n",
        "\n",
        "def get_most_frequent(n: int):\n",
        "  fdist = FreqDist(text1)\n",
        "  return fdist.most_common(n)\n",
        "\n",
        "def get_frequency(words: list[str]):\n",
        "  fdist = FreqDist(text1)\n",
        "  for word in words:\n",
        "    print(f\"{word}: {fdist[word]}\")\n",
        "\n",
        "### End\n",
        "\n",
        "print(get_most_frequent(10))\n",
        "get_frequency([\"with\", \"Moby\", \"fish\", \"whale\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
            "with: 1659\n",
            "Moby: 84\n",
            "fish: 133\n",
            "whale: 906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert get_most_frequent(5) == [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024)]"
      ],
      "metadata": {
        "id": "ttVE805EfZP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F39jB_1jAboe"
      },
      "source": [
        ":Some of the common words are actually punctuations and '**stop-words**'. They don't help us much with our text analysis, and therefore can be safely ignored.\n",
        "\n",
        "Luckily, NLTK supplies a list of stop words, and python has the punctuation built in into the string package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qne74woEATJo",
        "outputId": "18cc6305-75fe-4461-bca7-1d59e3799026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NWLKWf79z94",
        "outputId": "5410003d-4c3f-447e-d8a6-04e919a18ebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import string\n",
        "\n",
        "print(string.punctuation)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APuonB9pA1T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be679e81-5b8b-49c1-c407-713603595d48"
      },
      "source": [
        "### Write a function - get_most_frequent_filtered(n: int) - that returns the top n frequent words, after filtering out stop words and punctuation.\n",
        "\n",
        "def get_most_frequent_filtered(n: int):\n",
        "  fdist = FreqDist(text1)\n",
        "  filtered_words = []\n",
        "\n",
        "  for word in fdist.most_common():\n",
        "    if word[0].lower() not in stopwords.words('english') and word[0] not in string.punctuation and word[0] != \"--\":\n",
        "            filtered_words.append(word)\n",
        "\n",
        "  return filtered_words[:n]\n",
        "\n",
        "### End\n",
        "\n",
        "print(get_most_frequent_filtered(10))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('whale', 906), ('one', 889), ('like', 624), ('upon', 538), ('man', 508), ('ship', 507), ('Ahab', 501), ('.\"', 489), ('ye', 460), ('old', 436)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert get_most_frequent_filtered(5) == [('whale', 906), ('one', 889), ('like', 624), ('upon', 538), ('man', 508)]"
      ],
      "metadata": {
        "id": "Y17Fx8baf68k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGCb2GiEEuqM"
      },
      "source": [
        "FreqDist can be used even further. Let's analyse the text by the word length.\n",
        "\n",
        "Using python ['list-comprehension'](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) method, we can easily get a list of all the words by their lengths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBYSet5WEtcS",
        "outputId": "65c09b04-dba9-4797-a008-ea6808a6b6a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For convenience of reading, showing here only the first 30\n",
        "[len(w) for w in text1][:30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 6,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 11,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 6,\n",
              " 1,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 2,\n",
              " 10,\n",
              " 2,\n",
              " 4,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise #3 (Advanced): Length Frequency"
      ],
      "metadata": {
        "id": "gK9wjxkQbP2I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgHoCYA-FXDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb82a94-c309-41e9-a498-b2fa52e5ec64"
      },
      "source": [
        "### Write a code to calculate the words lengthes frequency inside `text1`.\n",
        "### Find out what those 20 (longest?) words are.\n",
        "### How many times do the 20 most lengthiest words appear in the text?\n",
        "\n",
        "word_lengths = [len(w) for w in text1]\n",
        "fdist_len = FreqDist(word_lengths)\n",
        "print(fdist_len.most_common())\n",
        "\n",
        "longest_words = sorted(set(text1), key=len, reverse=True)[:20]\n",
        "print(longest_words)\n",
        "\n",
        "count_longest_words = sum([text1.count(word) for word in longest_words])\n",
        "print(count_longest_words)\n",
        "\n",
        "### End\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399), (8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177), (15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]\n",
            "['uninterpenetratingly', 'characteristically', 'cannibalistically', 'superstitiousness', 'uncompromisedness', 'subterraneousness', 'uncomfortableness', 'preternaturalness', 'comprehensiveness', 'circumnavigations', 'indispensableness', 'simultaneousness', 'irresistibleness', 'apprehensiveness', 'circumnavigation', 'responsibilities', 'physiognomically', 'supernaturalness', 'indiscriminately', 'CIRCUMNAVIGATION']\n",
            "31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buil9-Z-Emvk"
      },
      "source": [
        "# Text Analysis: n-grams and collocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVKEjI4GCXei"
      },
      "source": [
        "As we saw in class, a word might not always also be a `token`. In the case of 'New York', 'ice cream', 'red wine', etc., every word meaning on its own is different than the combined meaning as a phrase.\n",
        "\n",
        "A **collocation** is a sequence of words that occur together unusually often.\n",
        "\n",
        "An `n-gram` is a sequence of a size of 'n' of tokens (i.e. words):\n",
        "\n",
        "* When n=1: it is called **unigram**\n",
        "* When n=2: it is called **bigram**\n",
        "* When n=3: it is called **trigram** ...\n",
        "* When n>3: it is just called an **n-gram** with the size of 4.\n",
        "\n",
        "\n",
        "NLTK has two functions: `bigrams` and `collocations`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itg3dYQ3DeSG",
        "outputId": "8671d391-2f6b-4e05-a961-64b659d8b20a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(bigrams([1,2,3,4,5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2), (2, 3), (3, 4), (4, 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HosjcKYuEIac",
        "outputId": "68cd5ed5-0dd8-49f7-ee85-d9cd773ae701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Bigrams generates bi-grams from the text: every two words would be collected together.\n",
        "\n",
        "list(bigrams(text1))[:20]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[', 'Moby'),\n",
              " ('Moby', 'Dick'),\n",
              " ('Dick', 'by'),\n",
              " ('by', 'Herman'),\n",
              " ('Herman', 'Melville'),\n",
              " ('Melville', '1851'),\n",
              " ('1851', ']'),\n",
              " (']', 'ETYMOLOGY'),\n",
              " ('ETYMOLOGY', '.'),\n",
              " ('.', '('),\n",
              " ('(', 'Supplied'),\n",
              " ('Supplied', 'by'),\n",
              " ('by', 'a'),\n",
              " ('a', 'Late'),\n",
              " ('Late', 'Consumptive'),\n",
              " ('Consumptive', 'Usher'),\n",
              " ('Usher', 'to'),\n",
              " ('to', 'a'),\n",
              " ('a', 'Grammar'),\n",
              " ('Grammar', 'School')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUqPHOytDfBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da75b0d-89e3-4baf-8969-bc2858b99f9c"
      },
      "source": [
        "### Your Turn ###\n",
        "# Write here code that returns and print the collocations in text1\n",
        "\n",
        "text1.collocations()\n",
        "\n",
        "# Expected output:\n",
        "# Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
        "# whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
        "# years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
        "# mate; white whale; ivory leg; one hand\n",
        "\n",
        "### END ###\n",
        "\n",
        "\"\"\"\n",
        "Here I also tried implementing the code manually. Of course the FreqDist function additionally outputs the frequencies integers, but\n",
        "strangely there are also other bigrams that do not appear in the output of the built-in .collocations() function and some frequencies\n",
        "seem to be off. I wonder how the .collocations() function works under the hood or what I did wrong here.\n",
        "\"\"\"\n",
        "\n",
        "def get_most_frequent_filtered_bigrams(n: int):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_text = [word for word in text1 if word.lower() not in stop_words and word not in string.punctuation and len(word) >= 3]\n",
        "  filtered_bigrams = [bigram for bigram in bigrams(filtered_text) if bigram[0].lower() not in stop_words and bigram[1].lower() not in stop_words]\n",
        "  bigram_frequencies = FreqDist(filtered_bigrams)\n",
        "  return bigram_frequencies.most_common(n)\n",
        "\n",
        "print(get_most_frequent_filtered_bigrams(20))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
            "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
            "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
            "mate; white whale; ivory leg; one hand\n",
            "[(('Sperm', 'Whale'), 119), (('Moby', 'Dick'), 84), (('old', 'man'), 80), (('White', 'Whale'), 74), (('Captain', 'Ahab'), 62), (('sperm', 'whale'), 56), (('mast', 'head'), 45), (('Right', 'Whale'), 39), (('mast', 'heads'), 36), (('whale', 'ship'), 33), (('cried', 'Ahab'), 33), (('Captain', 'Peleg'), 32), (('Aye', 'aye'), 31), (('white', 'whale'), 31), (('quarter', 'deck'), 28), (('one', 'hand'), 28), (('whale', 'boat'), 26), (('one', 'side'), 23), (('cried', 'Stubb'), 23), (('every', 'one'), 21)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zko0TVbLHWbl"
      },
      "source": [
        "# Python and NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2vqtYKYHaPa"
      },
      "source": [
        "Python has many strong capabilities, built in, when it comes to string and text procesing, combined with the list comprehension.\n",
        "\n",
        "Here are some examples of filtering the word list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWs-o3xoHZck",
        "outputId": "fbc14b78-4371-4b2b-84f5-bf1669f89c28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get all the words that end with 'ableness', sorted:\n",
        "sorted(w for w in set(text1) if w.endswith('ableness'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['comfortableness',\n",
              " 'honourableness',\n",
              " 'immutableness',\n",
              " 'indispensableness',\n",
              " 'indomitableness',\n",
              " 'intolerableness',\n",
              " 'palpableness',\n",
              " 'reasonableness',\n",
              " 'uncomfortableness']"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Dp1G3nHv3N",
        "outputId": "75e5f212-08a9-4662-fc3e-ed8ec0f49eb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get all the words that contain 'orate', sorted:\n",
        "sorted(term for term in set(text1) if 'orate' in term)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['camphorated',\n",
              " 'corroborated',\n",
              " 'decorated',\n",
              " 'elaborate',\n",
              " 'elaborately',\n",
              " 'evaporate',\n",
              " 'evaporates',\n",
              " 'incorporate',\n",
              " 'incorporated']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxVjcOg_H1e8",
        "outputId": "7d5595af-b8fd-463e-eb4b-b649ae0a4c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "source": [
        "# Get all the words whose first letter is capitalized:\n",
        "sorted(item for item in set(text1) if item.istitle())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3D',\n",
              " 'A',\n",
              " 'Abashed',\n",
              " 'Abednego',\n",
              " 'Abel',\n",
              " 'Abjectus',\n",
              " 'Aboard',\n",
              " 'Abominable',\n",
              " 'About',\n",
              " 'Above',\n",
              " 'Abraham',\n",
              " 'Academy',\n",
              " 'Accessory',\n",
              " 'According',\n",
              " 'Accordingly',\n",
              " 'Accursed',\n",
              " 'Achilles',\n",
              " 'Actium',\n",
              " 'Acushnet',\n",
              " 'Adam',\n",
              " 'Adieu',\n",
              " 'Adios',\n",
              " 'Admiral',\n",
              " 'Admirals',\n",
              " 'Advance',\n",
              " 'Advancement',\n",
              " 'Adventures',\n",
              " 'Adverse',\n",
              " 'Advocate',\n",
              " 'Affected',\n",
              " 'Affidavit',\n",
              " 'Affrighted',\n",
              " 'Afric',\n",
              " 'Africa',\n",
              " 'African',\n",
              " 'Africans',\n",
              " 'Aft',\n",
              " 'After',\n",
              " 'Afterwards',\n",
              " 'Again',\n",
              " 'Against',\n",
              " 'Agassiz',\n",
              " 'Ages',\n",
              " 'Ah',\n",
              " 'Ahab',\n",
              " 'Ahabs',\n",
              " 'Ahasuerus',\n",
              " 'Ahaz',\n",
              " 'Ahoy',\n",
              " 'Ain',\n",
              " 'Air',\n",
              " 'Akin',\n",
              " 'Alabama',\n",
              " 'Aladdin',\n",
              " 'Alarmed',\n",
              " 'Alas',\n",
              " 'Albatross',\n",
              " 'Albemarle',\n",
              " 'Albert',\n",
              " 'Albicore',\n",
              " 'Albino',\n",
              " 'Aldrovandi',\n",
              " 'Aldrovandus',\n",
              " 'Alexander',\n",
              " 'Alexanders',\n",
              " 'Alfred',\n",
              " 'Algerine',\n",
              " 'Algiers',\n",
              " 'Alike',\n",
              " 'Alive',\n",
              " 'All',\n",
              " 'Alleghanian',\n",
              " 'Alleghanies',\n",
              " 'Alley',\n",
              " 'Almanack',\n",
              " 'Almighty',\n",
              " 'Almost',\n",
              " 'Aloft',\n",
              " 'Alone',\n",
              " 'Alps',\n",
              " 'Already',\n",
              " 'Also',\n",
              " 'Am',\n",
              " 'Ambergriese',\n",
              " 'Ambergris',\n",
              " 'Amelia',\n",
              " 'America',\n",
              " 'American',\n",
              " 'Americans',\n",
              " 'Americas',\n",
              " 'Amittai',\n",
              " 'Among',\n",
              " 'Amsterdam',\n",
              " 'An',\n",
              " 'Anacharsis',\n",
              " 'Anak',\n",
              " 'Anatomist',\n",
              " 'And',\n",
              " 'Andes',\n",
              " 'Andrew',\n",
              " 'Andromeda',\n",
              " 'Angel',\n",
              " 'Angelo',\n",
              " 'Angels',\n",
              " 'Animated',\n",
              " 'Annawon',\n",
              " 'Anne',\n",
              " 'Anno',\n",
              " 'Anomalous',\n",
              " 'Another',\n",
              " 'Answer',\n",
              " 'Antarctic',\n",
              " 'Antilles',\n",
              " 'Antiochus',\n",
              " 'Antony',\n",
              " 'Antwerp',\n",
              " 'Anvil',\n",
              " 'Any',\n",
              " 'Anyhow',\n",
              " 'Anything',\n",
              " 'Anyway',\n",
              " 'Apollo',\n",
              " 'Apoplexy',\n",
              " 'Applied',\n",
              " 'Apply',\n",
              " 'April',\n",
              " 'Aquarius',\n",
              " 'Arch',\n",
              " 'Archbishop',\n",
              " 'Arched',\n",
              " 'Archer',\n",
              " 'Archipelagoes',\n",
              " 'Archy',\n",
              " 'Arctic',\n",
              " 'Are',\n",
              " 'Arethusa',\n",
              " 'Argo',\n",
              " 'Aries',\n",
              " 'Arion',\n",
              " 'Aristotle',\n",
              " 'Ark',\n",
              " 'Arkansas',\n",
              " 'Arkite',\n",
              " 'Arm',\n",
              " 'Armada',\n",
              " 'Arnold',\n",
              " 'Aroostook',\n",
              " 'Around',\n",
              " 'Arrayed',\n",
              " 'Arrived',\n",
              " 'Arsacidean',\n",
              " 'Arsacides',\n",
              " 'Art',\n",
              " 'Artedi',\n",
              " 'Arter',\n",
              " 'Articles',\n",
              " 'As',\n",
              " 'Asa',\n",
              " 'Ashantee',\n",
              " 'Ashore',\n",
              " 'Asia',\n",
              " 'Asiatic',\n",
              " 'Asiatics',\n",
              " 'Aside',\n",
              " 'Asphaltites',\n",
              " 'Assaulted',\n",
              " 'Assume',\n",
              " 'Assuming',\n",
              " 'Assuredly',\n",
              " 'Assyrian',\n",
              " 'Astern',\n",
              " 'Astir',\n",
              " 'Astronomy',\n",
              " 'At',\n",
              " 'Atlantic',\n",
              " 'Atlantics',\n",
              " 'Attached',\n",
              " 'Attend',\n",
              " 'August',\n",
              " 'Aunt',\n",
              " 'Australia',\n",
              " 'Australian',\n",
              " 'Austrian',\n",
              " 'Author',\n",
              " 'Authors',\n",
              " 'Auto',\n",
              " 'Availing',\n",
              " 'Avast',\n",
              " 'Avatar',\n",
              " 'Aware',\n",
              " 'Away',\n",
              " 'Awful',\n",
              " 'Ay',\n",
              " 'Aye',\n",
              " 'Azores',\n",
              " 'Babel',\n",
              " 'Babylon',\n",
              " 'Babylonian',\n",
              " 'Bachelor',\n",
              " 'Back',\n",
              " 'Backs',\n",
              " 'Bad',\n",
              " 'Baden',\n",
              " 'Bag',\n",
              " 'Balaene',\n",
              " 'Baliene',\n",
              " 'Baling',\n",
              " 'Bally',\n",
              " 'Baltic',\n",
              " 'Baltimore',\n",
              " 'Bamboo',\n",
              " 'Bang',\n",
              " 'Banks',\n",
              " 'Barbary',\n",
              " 'Bare',\n",
              " 'Bargain',\n",
              " 'Baron',\n",
              " 'Barrens',\n",
              " 'Bartholomew',\n",
              " 'Base',\n",
              " 'Bashaw',\n",
              " 'Bashee',\n",
              " 'Basilosaurus',\n",
              " 'Bastille',\n",
              " 'Battering',\n",
              " 'Battery',\n",
              " 'Bay',\n",
              " 'Bays',\n",
              " 'Be',\n",
              " 'Beach',\n",
              " 'Beale',\n",
              " 'Beams',\n",
              " 'Bear',\n",
              " 'Bears',\n",
              " 'Beat',\n",
              " 'Because',\n",
              " 'Becket',\n",
              " 'Bedford',\n",
              " 'Beelzebub',\n",
              " 'Befooled',\n",
              " 'Before',\n",
              " 'Begone',\n",
              " 'Behold',\n",
              " 'Behring',\n",
              " 'Being',\n",
              " 'Belated',\n",
              " 'Belial',\n",
              " 'Believe',\n",
              " 'Belisarius',\n",
              " 'Bell',\n",
              " 'Bellies',\n",
              " 'Beloved',\n",
              " 'Below',\n",
              " 'Belshazzar',\n",
              " 'Belubed',\n",
              " 'Bench',\n",
              " 'Bendigoes',\n",
              " 'Beneath',\n",
              " 'Bengal',\n",
              " 'Benjamin',\n",
              " 'Bennett',\n",
              " 'Bentham',\n",
              " 'Berkshire',\n",
              " 'Berlin',\n",
              " 'Bernard',\n",
              " 'Besides',\n",
              " 'Bess',\n",
              " 'Best',\n",
              " 'Bestow',\n",
              " 'Bethink',\n",
              " 'Better',\n",
              " 'Betty',\n",
              " 'Between',\n",
              " 'Beware',\n",
              " 'Beyond',\n",
              " 'Bible',\n",
              " 'Bibles',\n",
              " 'Bibliographical',\n",
              " 'Bildad',\n",
              " 'Biographical',\n",
              " 'Birmah',\n",
              " 'Bishop',\n",
              " 'Bite',\n",
              " 'Black',\n",
              " 'Blacksmith',\n",
              " 'Blackstone',\n",
              " 'Blanc',\n",
              " 'Blanche',\n",
              " 'Blanco',\n",
              " 'Blang',\n",
              " 'Blanket',\n",
              " 'Blast',\n",
              " 'Bless',\n",
              " 'Blind',\n",
              " 'Blinding',\n",
              " 'Blocksburg',\n",
              " 'Blood',\n",
              " 'Bloody',\n",
              " 'Blue',\n",
              " 'Boat',\n",
              " 'Boats',\n",
              " 'Bobbing',\n",
              " 'Bolivia',\n",
              " 'Bombay',\n",
              " 'Bonapartes',\n",
              " 'Bone',\n",
              " 'Bones',\n",
              " 'Bonneterre',\n",
              " 'Booble',\n",
              " 'Book',\n",
              " 'Boomer',\n",
              " 'Boone',\n",
              " 'Bordeaux',\n",
              " 'Borean',\n",
              " 'Born',\n",
              " 'Borneo',\n",
              " 'Bosom',\n",
              " 'Boston',\n",
              " 'Both',\n",
              " 'Bottle',\n",
              " 'Bottom',\n",
              " 'Bourbons',\n",
              " 'Bout',\n",
              " 'Bouton',\n",
              " 'Bowditch',\n",
              " 'Bower',\n",
              " 'Boy',\n",
              " 'Boys',\n",
              " 'Brace',\n",
              " 'Brahma',\n",
              " 'Brahmins',\n",
              " 'Brandreth',\n",
              " 'Brazil',\n",
              " 'Breakfast',\n",
              " 'Bremen',\n",
              " 'Bress',\n",
              " 'Bridge',\n",
              " 'Brighggians',\n",
              " 'Bright',\n",
              " 'Bring',\n",
              " 'Brisson',\n",
              " 'Brit',\n",
              " 'Britain',\n",
              " 'British',\n",
              " 'Britons',\n",
              " 'Broad',\n",
              " 'Broadway',\n",
              " 'Broke',\n",
              " 'Brother',\n",
              " 'Browne',\n",
              " 'Brute',\n",
              " 'Buckets',\n",
              " 'Bud',\n",
              " 'Buffalo',\n",
              " 'Bulkington',\n",
              " 'Bull',\n",
              " 'Bulwarks',\n",
              " 'Bunger',\n",
              " 'Bungle',\n",
              " 'Bunyan',\n",
              " 'Buoy',\n",
              " 'Buoyed',\n",
              " 'Burke',\n",
              " 'Burkes',\n",
              " 'Burst',\n",
              " 'Burton',\n",
              " 'Burtons',\n",
              " 'Business',\n",
              " 'But',\n",
              " 'Butchers',\n",
              " 'Butler',\n",
              " 'By',\n",
              " 'Byward',\n",
              " 'C',\n",
              " 'Cabaco',\n",
              " 'Cabin',\n",
              " 'Cachalot',\n",
              " 'Cadiz',\n",
              " 'Caesar',\n",
              " 'Caesarian',\n",
              " 'Cain',\n",
              " 'Calais',\n",
              " 'Californian',\n",
              " 'Call',\n",
              " 'Callao',\n",
              " 'Cambyses',\n",
              " 'Camel',\n",
              " 'Campagna',\n",
              " 'Can',\n",
              " 'Canaan',\n",
              " 'Canada',\n",
              " 'Canadian',\n",
              " 'Canal',\n",
              " 'Canaller',\n",
              " 'Canallers',\n",
              " 'Canals',\n",
              " 'Canaris',\n",
              " 'Cancer',\n",
              " 'Candles',\n",
              " 'Cannibal',\n",
              " 'Cannibals',\n",
              " 'Cannon',\n",
              " 'Canst',\n",
              " 'Cant',\n",
              " 'Canterbury',\n",
              " 'Cap',\n",
              " 'Cape',\n",
              " 'Capes',\n",
              " 'Capricornus',\n",
              " 'Captain',\n",
              " 'Captains',\n",
              " 'Capting',\n",
              " 'Caramba',\n",
              " 'Careful',\n",
              " 'Carefully',\n",
              " 'Carey',\n",
              " 'Carpenter',\n",
              " 'Carpet',\n",
              " 'Carrol',\n",
              " 'Carson',\n",
              " 'Carthage',\n",
              " 'Caryatid',\n",
              " 'Case',\n",
              " 'Cash',\n",
              " 'Cassock',\n",
              " 'Castaway',\n",
              " 'Castle',\n",
              " 'Categut',\n",
              " 'Cathedral',\n",
              " 'Catholic',\n",
              " 'Cato',\n",
              " 'Catskill',\n",
              " 'Cattegat',\n",
              " 'Caught',\n",
              " 'Cave',\n",
              " 'Caw',\n",
              " 'Cellini',\n",
              " 'Central',\n",
              " 'Certain',\n",
              " 'Certainly',\n",
              " 'Cervantes',\n",
              " 'Cetacea',\n",
              " 'Cetacean',\n",
              " 'Cetology',\n",
              " 'Cetus',\n",
              " 'Ceylon',\n",
              " 'Chace',\n",
              " 'Chaldee',\n",
              " 'Champagne',\n",
              " 'Champollion',\n",
              " 'Channel',\n",
              " 'Chapel',\n",
              " 'Charing',\n",
              " 'Charity',\n",
              " 'Charlemagne',\n",
              " 'Charley',\n",
              " 'Chart',\n",
              " 'Chartering',\n",
              " 'Chase',\n",
              " 'Cheever',\n",
              " 'Cherries',\n",
              " 'Chestnut',\n",
              " 'Chief',\n",
              " 'Childe',\n",
              " 'Chili',\n",
              " 'Chilian',\n",
              " 'China',\n",
              " 'Chinese',\n",
              " 'Cholo',\n",
              " 'Chowder',\n",
              " 'Christ',\n",
              " 'Christendom',\n",
              " 'Christian',\n",
              " 'Christianity',\n",
              " 'Christians',\n",
              " 'Christmas',\n",
              " 'Church',\n",
              " 'Cinque',\n",
              " 'Circassian',\n",
              " 'Circumambulate',\n",
              " 'Cistern',\n",
              " 'Civitas',\n",
              " 'Clam',\n",
              " 'Clap',\n",
              " 'Claus',\n",
              " 'Clay',\n",
              " 'Clear',\n",
              " 'Clearing',\n",
              " 'Cleopatra',\n",
              " 'Cleveland',\n",
              " 'Clifford',\n",
              " 'Clinging',\n",
              " 'Clootz',\n",
              " 'Close',\n",
              " 'Closing',\n",
              " 'Cloud',\n",
              " 'Cluny',\n",
              " 'Coast',\n",
              " 'Cock',\n",
              " 'Cockatoo',\n",
              " 'Cod',\n",
              " 'Cods',\n",
              " 'Coenties',\n",
              " 'Coffin',\n",
              " 'Coffins',\n",
              " 'Cognac',\n",
              " 'Coke',\n",
              " 'Cold',\n",
              " 'Coleman',\n",
              " 'Coleridge',\n",
              " 'College',\n",
              " 'Colnett',\n",
              " 'Cologne',\n",
              " 'Colonies',\n",
              " 'Colossus',\n",
              " 'Columbus',\n",
              " 'Come',\n",
              " 'Coming',\n",
              " 'Commanded',\n",
              " 'Commanders',\n",
              " 'Commend',\n",
              " 'Commodore',\n",
              " 'Commodores',\n",
              " 'Common',\n",
              " 'Commonly',\n",
              " 'Commons',\n",
              " 'Commonwealth',\n",
              " 'Companies',\n",
              " 'Comparing',\n",
              " 'Concerning',\n",
              " 'Congo',\n",
              " 'Congregation',\n",
              " 'Congregational',\n",
              " 'Conjuror',\n",
              " 'Connecticut',\n",
              " 'Consequently',\n",
              " 'Consider',\n",
              " 'Considering',\n",
              " 'Constable',\n",
              " 'Constantine',\n",
              " 'Constantinople',\n",
              " 'Consumptive',\n",
              " 'Continents',\n",
              " 'Contrasted',\n",
              " 'Conversation',\n",
              " 'Convulsively',\n",
              " 'Cook',\n",
              " 'Cooke',\n",
              " 'Cooks',\n",
              " 'Cooper',\n",
              " 'Coopman',\n",
              " 'Copenhagen',\n",
              " 'Coppered',\n",
              " 'Corinthians',\n",
              " 'Corkscrew',\n",
              " 'Corlaer',\n",
              " 'Corlears',\n",
              " 'Coronation',\n",
              " 'Corresponding',\n",
              " 'Corrupt',\n",
              " 'Cough',\n",
              " 'Could',\n",
              " 'Count',\n",
              " 'Counterpane',\n",
              " 'County',\n",
              " 'Court',\n",
              " 'Cousin',\n",
              " 'Cowper',\n",
              " 'Crab',\n",
              " 'Crack',\n",
              " 'Crammer',\n",
              " 'Crappo',\n",
              " 'Crappoes',\n",
              " 'Crazed',\n",
              " 'Creagh',\n",
              " 'Created',\n",
              " 'Cretan',\n",
              " 'Crete',\n",
              " 'Crew',\n",
              " 'Crish',\n",
              " 'Crockett',\n",
              " 'Cross',\n",
              " 'Crossed',\n",
              " 'Crossing',\n",
              " 'Crotch',\n",
              " 'Crowding',\n",
              " 'Crown',\n",
              " 'Crozetts',\n",
              " 'Cruelty',\n",
              " 'Cruising',\n",
              " 'Cruppered',\n",
              " 'Crusaders',\n",
              " 'Crushed',\n",
              " 'Crying',\n",
              " 'Cuba',\n",
              " 'Curious',\n",
              " 'Curse',\n",
              " 'Cursed',\n",
              " 'Curses',\n",
              " 'Cussed',\n",
              " 'Customs',\n",
              " 'Cut',\n",
              " 'Cutter',\n",
              " 'Cutting',\n",
              " 'Cuvier',\n",
              " 'Cyclades',\n",
              " 'Czar',\n",
              " 'D',\n",
              " 'Daboll',\n",
              " 'Daggoo',\n",
              " 'Dagon',\n",
              " 'Dame',\n",
              " 'Damn',\n",
              " 'Damocles',\n",
              " 'Dampier',\n",
              " 'Dan',\n",
              " 'Dance',\n",
              " 'Danes',\n",
              " 'Daniel',\n",
              " 'Danish',\n",
              " 'Dante',\n",
              " 'Dantean',\n",
              " 'Dar',\n",
              " 'Dardanelles',\n",
              " 'Darien',\n",
              " 'Darkness',\n",
              " 'Darmonodes',\n",
              " 'Dart',\n",
              " 'Dash',\n",
              " 'Dashing',\n",
              " 'Dauphine',\n",
              " 'Davis',\n",
              " 'Davy',\n",
              " 'Day',\n",
              " 'Days',\n",
              " 'De',\n",
              " 'Deacon',\n",
              " 'Dead',\n",
              " 'Death',\n",
              " 'Decanter',\n",
              " 'Decapitation',\n",
              " 'December',\n",
              " 'Deck',\n",
              " 'Deep',\n",
              " 'Deer',\n",
              " 'Deity',\n",
              " 'Del',\n",
              " 'Deliberately',\n",
              " 'Delight',\n",
              " 'Delightful',\n",
              " 'Deliverer',\n",
              " 'Delta',\n",
              " 'Den',\n",
              " 'Denderah',\n",
              " 'Depend',\n",
              " 'Derick',\n",
              " 'Dericks',\n",
              " 'Descartian',\n",
              " 'Descending',\n",
              " 'Desecrated',\n",
              " 'Desmarest',\n",
              " 'Desolation',\n",
              " 'Despairing',\n",
              " 'Despatch',\n",
              " 'Detached',\n",
              " 'Deuteronomy',\n",
              " 'Devil',\n",
              " 'Devils',\n",
              " 'Dey',\n",
              " 'Diaz',\n",
              " 'Dick',\n",
              " 'Did',\n",
              " 'Didn',\n",
              " 'Didst',\n",
              " 'Diminish',\n",
              " 'Ding',\n",
              " 'Dinner',\n",
              " 'Dinting',\n",
              " 'Discovery',\n",
              " 'Disdain',\n",
              " 'Dish',\n",
              " 'Dismal',\n",
              " 'Dissect',\n",
              " 'Dives',\n",
              " 'Divine',\n",
              " 'Diving',\n",
              " 'Do',\n",
              " 'Doctor',\n",
              " 'Dodge',\n",
              " 'Does',\n",
              " 'Doesn',\n",
              " 'Dog',\n",
              " 'Dolly',\n",
              " 'Dome',\n",
              " 'Dominic',\n",
              " 'Don',\n",
              " 'Dons',\n",
              " 'Doom',\n",
              " 'Dorchester',\n",
              " 'Dost',\n",
              " 'Doubloon',\n",
              " 'Doubtless',\n",
              " 'Doubts',\n",
              " 'Dough',\n",
              " 'Dover',\n",
              " 'Down',\n",
              " 'Dr',\n",
              " 'Dragged',\n",
              " 'Dragon',\n",
              " 'Drat',\n",
              " 'Drawing',\n",
              " 'Drawn',\n",
              " 'Draws',\n",
              " 'Drink',\n",
              " 'Drinking',\n",
              " 'Drive',\n",
              " 'Drop',\n",
              " 'Dropping',\n",
              " 'Dry',\n",
              " 'Duck',\n",
              " 'Dugongs',\n",
              " 'Duke',\n",
              " 'Dunder',\n",
              " 'Dunfermline',\n",
              " 'Dunkirk',\n",
              " 'Duodecimo',\n",
              " 'Duodecimoes',\n",
              " 'Durand',\n",
              " 'Durer',\n",
              " 'During',\n",
              " 'Dusk',\n",
              " 'Dut',\n",
              " 'Dutch',\n",
              " 'Dutchman',\n",
              " 'Dying',\n",
              " 'E',\n",
              " 'Each',\n",
              " 'Eagle',\n",
              " 'Earl',\n",
              " 'Earls',\n",
              " 'Earthsman',\n",
              " 'East',\n",
              " 'Eastern',\n",
              " 'Easy',\n",
              " 'Ebony',\n",
              " 'Ecclesiastes',\n",
              " 'Eckerman',\n",
              " 'Eddystone',\n",
              " 'Edgewise',\n",
              " 'Edmund',\n",
              " 'Edward',\n",
              " 'Ego',\n",
              " 'Egypt',\n",
              " 'Egyptian',\n",
              " 'Egyptians',\n",
              " 'Eh',\n",
              " 'Ehrenbreitstein',\n",
              " 'Eight',\n",
              " 'Either',\n",
              " 'Elbe',\n",
              " 'Electors',\n",
              " 'Elephant',\n",
              " 'Elephanta',\n",
              " 'Elephants',\n",
              " 'Elijah',\n",
              " 'Ellenborough',\n",
              " 'Elsewhere',\n",
              " 'Emblazonings',\n",
              " 'Emboldened',\n",
              " 'Emir',\n",
              " 'Emperor',\n",
              " 'Emperors',\n",
              " 'Empire',\n",
              " 'End',\n",
              " 'Enderbies',\n",
              " 'Enderby',\n",
              " 'Enderbys',\n",
              " 'England',\n",
              " 'Englander',\n",
              " 'English',\n",
              " 'Englishman',\n",
              " 'Englishmen',\n",
              " 'Enough',\n",
              " 'Enter',\n",
              " 'Entering',\n",
              " 'Entreaties',\n",
              " 'Enveloped',\n",
              " 'Ephesian',\n",
              " 'Epilogue',\n",
              " 'Epitome',\n",
              " 'Equality',\n",
              " 'Equator',\n",
              " 'Equatorial',\n",
              " 'Ere',\n",
              " 'Erie',\n",
              " 'Erromanggoans',\n",
              " 'Erroneous',\n",
              " 'Erskine',\n",
              " 'Esau',\n",
              " 'Espied',\n",
              " 'Espying',\n",
              " 'Esquimaux',\n",
              " 'Essex',\n",
              " 'Et',\n",
              " 'Eternities',\n",
              " 'Eternity',\n",
              " 'Ethiopian',\n",
              " 'Euclid',\n",
              " 'Euclidean',\n",
              " 'Euroclydon',\n",
              " 'Europa',\n",
              " 'Europe',\n",
              " 'European',\n",
              " 'Evangelist',\n",
              " 'Evangelists',\n",
              " 'Even',\n",
              " 'Ever',\n",
              " 'Every',\n",
              " 'Evil',\n",
              " 'Ex',\n",
              " 'Excellent',\n",
              " 'Excepting',\n",
              " 'Exception',\n",
              " 'Excuse',\n",
              " 'Expedition',\n",
              " 'Expeditions',\n",
              " 'Explain',\n",
              " 'Exploring',\n",
              " 'Extending',\n",
              " 'Ezekiel',\n",
              " 'F',\n",
              " 'Fa',\n",
              " 'Face',\n",
              " 'Fain',\n",
              " 'Faintly',\n",
              " 'Fair',\n",
              " 'Faith',\n",
              " 'Falsehood',\n",
              " 'Fanning',\n",
              " 'Far',\n",
              " 'Farewell',\n",
              " 'Fashioned',\n",
              " 'Fast',\n",
              " 'Fasting',\n",
              " 'Fat',\n",
              " 'Fata',\n",
              " 'Fate',\n",
              " 'Fates',\n",
              " 'Father',\n",
              " 'Fe',\n",
              " 'Fear',\n",
              " 'Fearing',\n",
              " 'February',\n",
              " 'Fedallah',\n",
              " 'Feegee',\n",
              " 'Feegeeans',\n",
              " 'Feegees',\n",
              " 'Feel',\n",
              " 'Feet',\n",
              " 'Fejee',\n",
              " 'Fellow',\n",
              " 'Ferdinando',\n",
              " 'Fernandes',\n",
              " 'Fetch',\n",
              " 'Few',\n",
              " 'Fields',\n",
              " 'Fiercely',\n",
              " 'Fiery',\n",
              " 'Fife',\n",
              " 'Fifth',\n",
              " 'Figuera',\n",
              " 'Fill',\n",
              " 'Fin',\n",
              " 'Finally',\n",
              " 'Find',\n",
              " 'Finding',\n",
              " 'Fine',\n",
              " 'Fired',\n",
              " 'First',\n",
              " 'Fish',\n",
              " 'Fisheries',\n",
              " 'Fishery',\n",
              " 'Fishes',\n",
              " 'Fishiest',\n",
              " 'Fits',\n",
              " 'Fitz',\n",
              " 'Five',\n",
              " 'Flask',\n",
              " 'Flat',\n",
              " 'Fleece',\n",
              " 'Fleet',\n",
              " 'Flip',\n",
              " 'Floating',\n",
              " 'Floundered',\n",
              " 'Flounders',\n",
              " 'Flukes',\n",
              " 'Flying',\n",
              " 'Fogo',\n",
              " 'Folding',\n",
              " 'Folger',\n",
              " 'Folgers',\n",
              " 'Folio',\n",
              " 'Folios',\n",
              " 'Fool',\n",
              " 'Foolish',\n",
              " 'For',\n",
              " 'Forced',\n",
              " 'Fore',\n",
              " 'Forecastle',\n",
              " 'Forehead',\n",
              " 'Foremost',\n",
              " 'Forge',\n",
              " 'Form',\n",
              " 'Forming',\n",
              " 'Formosa',\n",
              " 'Forthwith',\n",
              " 'Forty',\n",
              " 'Forward',\n",
              " 'Fossil',\n",
              " 'Fountain',\n",
              " 'Fourth',\n",
              " 'France',\n",
              " 'Frankfort',\n",
              " 'Franklin',\n",
              " 'Frederick',\n",
              " 'Free',\n",
              " 'Freely',\n",
              " 'Freeze',\n",
              " 'French',\n",
              " 'Frenchman',\n",
              " 'Frenchmen',\n",
              " 'Friar',\n",
              " 'Friend',\n",
              " 'Friends',\n",
              " 'Friesland',\n",
              " 'Frighted',\n",
              " 'Frobisher',\n",
              " 'Froissart',\n",
              " 'From',\n",
              " 'Fuego',\n",
              " 'Full',\n",
              " 'Funeral',\n",
              " 'Furl',\n",
              " 'Further',\n",
              " 'Furthermore',\n",
              " 'Future',\n",
              " 'Gabriel',\n",
              " 'Gaining',\n",
              " 'Gall',\n",
              " 'Galleries',\n",
              " 'Gallipagos',\n",
              " 'Gam',\n",
              " 'Gamming',\n",
              " 'Ganders',\n",
              " 'Ganges',\n",
              " 'Gardiner',\n",
              " 'Garnery',\n",
              " 'Gases',\n",
              " 'Gate',\n",
              " 'Gather',\n",
              " 'Gay',\n",
              " 'Gayer',\n",
              " 'Gayhead',\n",
              " 'Gazette',\n",
              " 'Gemini',\n",
              " 'General',\n",
              " 'Genesis',\n",
              " 'Geneva',\n",
              " 'Genius',\n",
              " 'Gentlemen',\n",
              " 'Gently',\n",
              " 'Geological',\n",
              " 'George',\n",
              " 'Ger',\n",
              " 'Germain',\n",
              " 'German',\n",
              " 'Germans',\n",
              " 'Gesner',\n",
              " 'Get',\n",
              " 'Ghent',\n",
              " 'Gibraltar',\n",
              " 'Gifted',\n",
              " 'Gilder',\n",
              " 'Ginger',\n",
              " 'Give',\n",
              " 'Giver',\n",
              " 'Giving',\n",
              " 'Glacier',\n",
              " 'Glancing',\n",
              " 'Glen',\n",
              " 'Gliding',\n",
              " 'Glimpses',\n",
              " 'Globe',\n",
              " 'Glory',\n",
              " 'Gnawed',\n",
              " 'Go',\n",
              " 'Goa',\n",
              " 'Goat',\n",
              " 'God',\n",
              " 'Gods',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YWHbsQyIhC4"
      },
      "source": [
        "And there are more. if `wrd` is a string, then, for example:\n",
        "\n",
        "* `wrd.islower()` will return true if the word is all lowercase\n",
        "* `wrd.isalpha()` will return true if all the characters in the string are letters\n",
        "\n",
        "and there are also: `wrd.startswith('str')`, `wrd.isdigit()`, `wr.isalnum()`\n",
        "and [many others](https://www.w3schools.com/python/python_ref_string.asp)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise #4: Functions and substrings search"
      ],
      "metadata": {
        "id": "jl2mxc9xa_nC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiP7qcUTIUS6",
        "outputId": "bd355ee0-979e-4194-a454-0c7d3740e868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from typing import List\n",
        "\n",
        "### Exercise:\n",
        "\n",
        "def detect_string(tokens: List[str], search_str: str, search_position: int = 0) -> List[str]:\n",
        "  \"\"\"\n",
        "  Returns a sorted list of the vocabulary tokens which match the search conditions\n",
        "\n",
        "  params:\n",
        "    tokens: a document tokens list.\n",
        "    search_str: a string to search in the token list\n",
        "    search_position: one of the following:\n",
        "      0 - anywhere in the string\n",
        "      1 - searches for the string at the beginning of the token\n",
        "      2 - searches for the string at the end of the token\n",
        "  \"\"\"\n",
        "  if search_position == 0:\n",
        "    return sorted(set([token for token in tokens if search_str in token]))\n",
        "  elif search_position == 1:\n",
        "    return sorted(set([token for token in tokens if token.startswith(search_str)]))\n",
        "  elif search_position == 2:\n",
        "    return sorted(set([token for token in tokens if token.endswith(search_str)]))\n",
        "  else:\n",
        "    return []\n",
        "\n",
        "print(detect_string(text1, 'tably', 2))\n",
        "print(detect_string(text1, 'argu', 1))\n",
        "print(detect_string(text1, 'arg', 2))\n",
        "print(detect_string(text1, 'larg', 0))\n",
        "\n",
        "  ### Fill in this function to return the result of searching for the\n",
        "  ### given string \"search_str\" in the token vocabulary \"tokens\", according to\n",
        "  ### the position parameter, as explained in the docstring\n",
        "\n",
        "###"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comfortably', 'discreditably', 'illimitably', 'immutably', 'indubitably', 'inevitably', 'inscrutably', 'profitably', 'unaccountably', 'unwarrantably']\n",
            "['argue', 'argued', 'arguing', 'argument', 'arguments']\n",
            "[]\n",
            "['enlarge', 'enlarged', 'enlarges', 'large', 'largely', 'largeness', 'larger', 'largest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_t95CzIL27v"
      },
      "source": [
        "### Test:\n",
        "assert detect_string(text1, 'tably', 2) == ['comfortably',\n",
        " 'discreditably',\n",
        " 'illimitably',\n",
        " 'immutably',\n",
        " 'indubitably',\n",
        " 'inevitably',\n",
        " 'inscrutably',\n",
        " 'profitably',\n",
        " 'unaccountably',\n",
        " 'unwarrantably']"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrBjzeHIL4_V"
      },
      "source": [
        "### Test:\n",
        "assert detect_string(text1, 'argu', 1) == ['argue', 'argued', 'arguing', 'argument', 'arguments']"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOJPKwpnL6nQ"
      },
      "source": [
        "### Test:\n",
        "assert detect_string(text1, 'arg', 2) == []"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8V2lvmWL8TC"
      },
      "source": [
        "### Test\n",
        "assert detect_string(text1, 'larg') == ['enlarge',\n",
        " 'enlarged',\n",
        " 'enlarges',\n",
        " 'large',\n",
        " 'largely',\n",
        " 'largeness',\n",
        " 'larger',\n",
        " 'largest']"
      ],
      "execution_count": 59,
      "outputs": []
    }
  ]
}
